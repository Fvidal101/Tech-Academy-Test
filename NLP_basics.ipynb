{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fvidal101/Tech-Academy-Test/blob/master/NLP_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jwzJ20D8eRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install nltk\n",
        "# if you are getting error for nltk, then install nltk using above script"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LGI6blB_x9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbhrfA8VBRcf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Error\n",
        "---------------------------------------------------------------------------\n",
        "LookupError                               Traceback (most recent call last)\n",
        "<ipython-input-4-65208e546d17> in <module>()\n",
        "      1 from nltk.tokenize import word_tokenize\n",
        "      2 \n",
        "----> 3 tokens = word_tokenize(sentence)\n",
        "\n",
        "4 frames\n",
        "/usr/local/lib/python3.6/dist-packages/nltk/data.py in find(resource_name, paths)\n",
        "    671     sep = '*' * 70\n",
        "    672     resource_not_found = '\\n%s\\n%s\\n%s\\n' % (sep, msg, sep)\n",
        "--> 673     raise LookupError(resource_not_found)\n",
        "    674 \n",
        "    675 \n",
        "\n",
        "LookupError: \n",
        "**********************************************************************\n",
        "  Resource punkt not found.\n",
        "  Please use the NLTK Downloader to obtain the resource:\n",
        "\n",
        "  >>> import nltk\n",
        "  >>> nltk.download('punkt')\n",
        "  \n",
        "  Searched in:\n",
        "    - '/root/nltk_data'\n",
        "    - '/usr/share/nltk_data'\n",
        "    - '/usr/local/share/nltk_data'\n",
        "    - '/usr/lib/nltk_data'\n",
        "    - '/usr/local/lib/nltk_data'\n",
        "    - '/usr/nltk_data'\n",
        "    - '/usr/lib/nltk_data'\n",
        "    - ''\n",
        "***********************************************"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58MaUxwRBMHG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c38964ff-6cc4-4365-a08c-5b4a2757b8a9"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW0-aD3BAFJn",
        "colab_type": "text"
      },
      "source": [
        "##Tokenization##\n",
        "1. Process of breaking the sentence into words </br>\n",
        "2. Structures the input senstence; data representation </br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2UmeAVeAoYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence  = \"We all are in the Techademy class. And this class belongs to artificial intelligence. We are learning artificial intelligence including machine learning , #NLP, #deep learning, #computer vision and various other applications of artificial intelligence. This course is useful.\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teKWrD3IA-Gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(sentence)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "210MamVtBF5x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "96265886-0729-47c2-fbe9-c137b18d6155"
      },
      "source": [
        "tokens"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We',\n",
              " 'all',\n",
              " 'are',\n",
              " 'in',\n",
              " 'the',\n",
              " 'Techademy',\n",
              " 'class',\n",
              " '.',\n",
              " 'And',\n",
              " 'this',\n",
              " 'class',\n",
              " 'belongs',\n",
              " 'to',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " '.',\n",
              " 'We',\n",
              " 'are',\n",
              " 'learning',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'including',\n",
              " 'machine',\n",
              " 'learning',\n",
              " ',',\n",
              " '#',\n",
              " 'NLP',\n",
              " ',',\n",
              " '#',\n",
              " 'deep',\n",
              " 'learning',\n",
              " ',',\n",
              " '#',\n",
              " 'computer',\n",
              " 'vision',\n",
              " 'and',\n",
              " 'various',\n",
              " 'other',\n",
              " 'applications',\n",
              " 'of',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " '.',\n",
              " 'This',\n",
              " 'course',\n",
              " 'is',\n",
              " 'useful',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icUPgfrp2u6_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9a50d64-5767-4ae7-f890-fe165c93d5b6"
      },
      "source": [
        "type(tokens)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRZF6vP9BhxA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d9a31cf-bcb4-47f2-eaf1-ba4ce0118fab"
      },
      "source": [
        "#Counting the word length\n",
        "len(tokens)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q88rH3GQBopH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "ed5b71e2-7d8a-4609-bd8f-65c04a6deece"
      },
      "source": [
        "# counting the word count in the sentence\n",
        "from nltk.probability import FreqDist\n",
        "fdist = FreqDist()\n",
        "\n",
        "for word in tokens:\n",
        "  fdist[word.lower()]+= 1       #tokens are convereted in lowercase\n",
        "\n",
        "fdist\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'#': 3,\n",
              "          ',': 3,\n",
              "          '.': 4,\n",
              "          'all': 1,\n",
              "          'and': 2,\n",
              "          'applications': 1,\n",
              "          'are': 2,\n",
              "          'artificial': 3,\n",
              "          'belongs': 1,\n",
              "          'class': 2,\n",
              "          'computer': 1,\n",
              "          'course': 1,\n",
              "          'deep': 1,\n",
              "          'in': 1,\n",
              "          'including': 1,\n",
              "          'intelligence': 3,\n",
              "          'is': 1,\n",
              "          'learning': 3,\n",
              "          'machine': 1,\n",
              "          'nlp': 1,\n",
              "          'of': 1,\n",
              "          'other': 1,\n",
              "          'techademy': 1,\n",
              "          'the': 1,\n",
              "          'this': 2,\n",
              "          'to': 1,\n",
              "          'useful': 1,\n",
              "          'various': 1,\n",
              "          'vision': 1,\n",
              "          'we': 2})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfYzjO_gCQTY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae8e36cb-1e64-43a8-e0bf-85e9e518e7ce"
      },
      "source": [
        "# most common used chracters in the sentence\n",
        "\n",
        "fdist_top3 = fdist.most_common(5)\n",
        "fdist_top3"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 4), ('artificial', 3), ('intelligence', 3), ('learning', 3), (',', 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW--9VZ-CzUu",
        "colab_type": "text"
      },
      "source": [
        "**Tokenisation types** </br>\n",
        "Ngram: tokens of N consecutive words </br>\n",
        "Bigram: tokens with 2 consecutive words. </br>\n",
        "Trigram: tokens with 3 consecutive words </br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHhFRq3yCthy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.util import bigrams, trigrams, ngrams"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZgPVf2oDp3_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "66453add-8516-44b4-8aee-7e4c9c0235b1"
      },
      "source": [
        "sent_bigrams = list(bigrams(tokens))\n",
        "sent_bigrams"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('We', 'all'),\n",
              " ('all', 'are'),\n",
              " ('are', 'in'),\n",
              " ('in', 'the'),\n",
              " ('the', 'Techademy'),\n",
              " ('Techademy', 'class'),\n",
              " ('class', '.'),\n",
              " ('.', 'And'),\n",
              " ('And', 'this'),\n",
              " ('this', 'class'),\n",
              " ('class', 'belongs'),\n",
              " ('belongs', 'to'),\n",
              " ('to', 'artificial'),\n",
              " ('artificial', 'intelligence'),\n",
              " ('intelligence', '.'),\n",
              " ('.', 'We'),\n",
              " ('We', 'are'),\n",
              " ('are', 'learning'),\n",
              " ('learning', 'artificial'),\n",
              " ('artificial', 'intelligence'),\n",
              " ('intelligence', 'including'),\n",
              " ('including', 'machine'),\n",
              " ('machine', 'learning'),\n",
              " ('learning', ','),\n",
              " (',', '#'),\n",
              " ('#', 'NLP'),\n",
              " ('NLP', ','),\n",
              " (',', '#'),\n",
              " ('#', 'deep'),\n",
              " ('deep', 'learning'),\n",
              " ('learning', ','),\n",
              " (',', '#'),\n",
              " ('#', 'computer'),\n",
              " ('computer', 'vision'),\n",
              " ('vision', 'and'),\n",
              " ('and', 'various'),\n",
              " ('various', 'other'),\n",
              " ('other', 'applications'),\n",
              " ('applications', 'of'),\n",
              " ('of', 'artificial'),\n",
              " ('artificial', 'intelligence'),\n",
              " ('intelligence', '.'),\n",
              " ('.', 'This'),\n",
              " ('This', 'course'),\n",
              " ('course', 'is'),\n",
              " ('is', 'useful'),\n",
              " ('useful', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG9vhwj3D8o9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "56424701-92bb-49fc-dd58-bf0ed26e0887"
      },
      "source": [
        "sent_trigrams = list(trigrams(tokens))\n",
        "sent_trigrams"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('We', 'all', 'are'),\n",
              " ('all', 'are', 'in'),\n",
              " ('are', 'in', 'the'),\n",
              " ('in', 'the', 'Techademy'),\n",
              " ('the', 'Techademy', 'class'),\n",
              " ('Techademy', 'class', '.'),\n",
              " ('class', '.', 'And'),\n",
              " ('.', 'And', 'this'),\n",
              " ('And', 'this', 'class'),\n",
              " ('this', 'class', 'belongs'),\n",
              " ('class', 'belongs', 'to'),\n",
              " ('belongs', 'to', 'artificial'),\n",
              " ('to', 'artificial', 'intelligence'),\n",
              " ('artificial', 'intelligence', '.'),\n",
              " ('intelligence', '.', 'We'),\n",
              " ('.', 'We', 'are'),\n",
              " ('We', 'are', 'learning'),\n",
              " ('are', 'learning', 'artificial'),\n",
              " ('learning', 'artificial', 'intelligence'),\n",
              " ('artificial', 'intelligence', 'including'),\n",
              " ('intelligence', 'including', 'machine'),\n",
              " ('including', 'machine', 'learning'),\n",
              " ('machine', 'learning', ','),\n",
              " ('learning', ',', '#'),\n",
              " (',', '#', 'NLP'),\n",
              " ('#', 'NLP', ','),\n",
              " ('NLP', ',', '#'),\n",
              " (',', '#', 'deep'),\n",
              " ('#', 'deep', 'learning'),\n",
              " ('deep', 'learning', ','),\n",
              " ('learning', ',', '#'),\n",
              " (',', '#', 'computer'),\n",
              " ('#', 'computer', 'vision'),\n",
              " ('computer', 'vision', 'and'),\n",
              " ('vision', 'and', 'various'),\n",
              " ('and', 'various', 'other'),\n",
              " ('various', 'other', 'applications'),\n",
              " ('other', 'applications', 'of'),\n",
              " ('applications', 'of', 'artificial'),\n",
              " ('of', 'artificial', 'intelligence'),\n",
              " ('artificial', 'intelligence', '.'),\n",
              " ('intelligence', '.', 'This'),\n",
              " ('.', 'This', 'course'),\n",
              " ('This', 'course', 'is'),\n",
              " ('course', 'is', 'useful'),\n",
              " ('is', 'useful', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISQW8NLaEBg3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "7e6c83a4-d3ea-4bde-c8b4-31f06e3579ca"
      },
      "source": [
        "# ngrams(tokens,N)       Fill the value of N\n",
        "sent_ngrams = list(ngrams(tokens,6))\n",
        "sent_ngrams"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('We', 'all', 'are', 'in', 'the', 'Techademy'),\n",
              " ('all', 'are', 'in', 'the', 'Techademy', 'class'),\n",
              " ('are', 'in', 'the', 'Techademy', 'class', '.'),\n",
              " ('in', 'the', 'Techademy', 'class', '.', 'And'),\n",
              " ('the', 'Techademy', 'class', '.', 'And', 'this'),\n",
              " ('Techademy', 'class', '.', 'And', 'this', 'class'),\n",
              " ('class', '.', 'And', 'this', 'class', 'belongs'),\n",
              " ('.', 'And', 'this', 'class', 'belongs', 'to'),\n",
              " ('And', 'this', 'class', 'belongs', 'to', 'artificial'),\n",
              " ('this', 'class', 'belongs', 'to', 'artificial', 'intelligence'),\n",
              " ('class', 'belongs', 'to', 'artificial', 'intelligence', '.'),\n",
              " ('belongs', 'to', 'artificial', 'intelligence', '.', 'We'),\n",
              " ('to', 'artificial', 'intelligence', '.', 'We', 'are'),\n",
              " ('artificial', 'intelligence', '.', 'We', 'are', 'learning'),\n",
              " ('intelligence', '.', 'We', 'are', 'learning', 'artificial'),\n",
              " ('.', 'We', 'are', 'learning', 'artificial', 'intelligence'),\n",
              " ('We', 'are', 'learning', 'artificial', 'intelligence', 'including'),\n",
              " ('are', 'learning', 'artificial', 'intelligence', 'including', 'machine'),\n",
              " ('learning',\n",
              "  'artificial',\n",
              "  'intelligence',\n",
              "  'including',\n",
              "  'machine',\n",
              "  'learning'),\n",
              " ('artificial', 'intelligence', 'including', 'machine', 'learning', ','),\n",
              " ('intelligence', 'including', 'machine', 'learning', ',', '#'),\n",
              " ('including', 'machine', 'learning', ',', '#', 'NLP'),\n",
              " ('machine', 'learning', ',', '#', 'NLP', ','),\n",
              " ('learning', ',', '#', 'NLP', ',', '#'),\n",
              " (',', '#', 'NLP', ',', '#', 'deep'),\n",
              " ('#', 'NLP', ',', '#', 'deep', 'learning'),\n",
              " ('NLP', ',', '#', 'deep', 'learning', ','),\n",
              " (',', '#', 'deep', 'learning', ',', '#'),\n",
              " ('#', 'deep', 'learning', ',', '#', 'computer'),\n",
              " ('deep', 'learning', ',', '#', 'computer', 'vision'),\n",
              " ('learning', ',', '#', 'computer', 'vision', 'and'),\n",
              " (',', '#', 'computer', 'vision', 'and', 'various'),\n",
              " ('#', 'computer', 'vision', 'and', 'various', 'other'),\n",
              " ('computer', 'vision', 'and', 'various', 'other', 'applications'),\n",
              " ('vision', 'and', 'various', 'other', 'applications', 'of'),\n",
              " ('and', 'various', 'other', 'applications', 'of', 'artificial'),\n",
              " ('various', 'other', 'applications', 'of', 'artificial', 'intelligence'),\n",
              " ('other', 'applications', 'of', 'artificial', 'intelligence', '.'),\n",
              " ('applications', 'of', 'artificial', 'intelligence', '.', 'This'),\n",
              " ('of', 'artificial', 'intelligence', '.', 'This', 'course'),\n",
              " ('artificial', 'intelligence', '.', 'This', 'course', 'is'),\n",
              " ('intelligence', '.', 'This', 'course', 'is', 'useful'),\n",
              " ('.', 'This', 'course', 'is', 'useful', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsdGAcjdEXO3",
        "colab_type": "text"
      },
      "source": [
        "##Stemming##\n",
        "Transform the words into its root form </br>\n",
        "having ---> have"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgJ1HzviEmK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "pst = PorterStemmer()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLM0lZq6E07p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "50c88d1b-e8df-4b70-efb4-7debacb7d9a2"
      },
      "source": [
        "pst.stem('having') # root word for having "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'have'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOISWQ7sE_y0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8076c565-e6a1-4f43-fff1-b17e5f32d765"
      },
      "source": [
        "stem_words= ['give','giving','given','gave']\n",
        "for words in stem_words:\n",
        "  print(words + \":\" + pst.stem(words))       "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "give:give\n",
            "giving:give\n",
            "given:given\n",
            "gave:gave\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKIlowtsGbyH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f0293d4f-5eba-4a8c-fa71-c87e0b5df63b"
      },
      "source": [
        "stem_words_1= ['go', 'went', 'gone']\n",
        "for words in stem_words_1:\n",
        "  print(words + \":\" + pst.stem(words))     "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "go:go\n",
            "went:went\n",
            "gone:gone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuqBEgcHFmh6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8dd9214b-889f-42ea-f77a-ec636059fe2c"
      },
      "source": [
        "# another form of stemmer\n",
        "\n",
        "from nltk.stem import LancasterStemmer\n",
        "lst = LancasterStemmer()\n",
        "\n",
        "for words in stem_words:\n",
        "  print(words + \":\" + lst.stem(words))  \n",
        "\n",
        "# bit more complex  "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "give:giv\n",
            "giving:giv\n",
            "given:giv\n",
            "gave:gav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daflACQ0LEhh",
        "colab_type": "text"
      },
      "source": [
        "##POS: Parts of Speech##\n",
        "1. Process grammar of words\n",
        "2. Helpful in the meaning for word\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtLbfNRUMfY0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5a94e507-a2ba-4836-f74b-4073f065c64b"
      },
      "source": [
        "nltk.download('tagsets')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc1G5Me4Mcbu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cd0859e8-336c-4ee8-8c88-91af6726e58c"
      },
      "source": [
        "# You can see different tags for the English Grammer using this script\n",
        "nltk.help.upenn_tagset()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPGZCEXmMvNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import pos_tag"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiCKYu38NAF7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "aebc9aec-e979-4e1f-da7b-a15aaa3c1efe"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-9-ajThM3Mn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "2f4d4ab6-ef04-4a75-ef0c-f991b38992f7"
      },
      "source": [
        "for token in tokens: \n",
        "  print(pos_tag([token]))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('We', 'PRP')]\n",
            "[('all', 'DT')]\n",
            "[('are', 'VBP')]\n",
            "[('in', 'IN')]\n",
            "[('the', 'DT')]\n",
            "[('Techademy', 'NN')]\n",
            "[('class', 'NN')]\n",
            "[('.', '.')]\n",
            "[('And', 'CC')]\n",
            "[('this', 'DT')]\n",
            "[('class', 'NN')]\n",
            "[('belongs', 'NNS')]\n",
            "[('to', 'TO')]\n",
            "[('artificial', 'JJ')]\n",
            "[('intelligence', 'NN')]\n",
            "[('.', '.')]\n",
            "[('We', 'PRP')]\n",
            "[('are', 'VBP')]\n",
            "[('learning', 'VBG')]\n",
            "[('artificial', 'JJ')]\n",
            "[('intelligence', 'NN')]\n",
            "[('including', 'VBG')]\n",
            "[('machine', 'NN')]\n",
            "[('learning', 'VBG')]\n",
            "[(',', ',')]\n",
            "[('#', '#')]\n",
            "[('NLP', 'NN')]\n",
            "[(',', ',')]\n",
            "[('#', '#')]\n",
            "[('deep', 'NN')]\n",
            "[('learning', 'VBG')]\n",
            "[(',', ',')]\n",
            "[('#', '#')]\n",
            "[('computer', 'NN')]\n",
            "[('vision', 'NN')]\n",
            "[('and', 'CC')]\n",
            "[('various', 'JJ')]\n",
            "[('other', 'JJ')]\n",
            "[('applications', 'NNS')]\n",
            "[('of', 'IN')]\n",
            "[('artificial', 'JJ')]\n",
            "[('intelligence', 'NN')]\n",
            "[('.', '.')]\n",
            "[('This', 'DT')]\n",
            "[('course', 'NN')]\n",
            "[('is', 'VBZ')]\n",
            "[('useful', 'JJ')]\n",
            "[('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50x6fU7CNElf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8469dce4-6cc0-4848-fb64-6af70566b802"
      },
      "source": [
        "new_sent = \"Dilanga is teaching AI class\"\n",
        "new_sent = word_tokenize(new_sent)\n",
        "for token in new_sent: \n",
        "  print(pos_tag([token]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Dilanga', 'NN')]\n",
            "[('is', 'VBZ')]\n",
            "[('teaching', 'VBG')]\n",
            "[('AI', 'NN')]\n",
            "[('class', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7AKd-pJF_xU",
        "colab_type": "text"
      },
      "source": [
        "##Lemmatization##\n",
        "1. Process morphological analysis of the word </br>\n",
        "2. Similar to stemming, maps words to one common root </br>\n",
        "3. Ouput is a proper word, hence required a detailed dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkBdStxIGm17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import wordnet # detailed dictionary\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "word_lemma = WordNetLemmatizer()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnPleqhIIS78",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f5a248d2-9cc5-423e-d377-fbbb5c1908d4"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tisvo3JWH94R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "a8cf9e3a-5cc5-4352-f194-ca6f6455efe4"
      },
      "source": [
        "word_lemma.lemmatize('stemming')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'stemming'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4xt9r8ONvSn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "3609fc8e-4238-441d-8b86-c9a416b13478"
      },
      "source": [
        "word_lemma.lemmatize('stemming', 'v')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'stem'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y2HSnjEIYQC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4e10be28-d130-4c54-fc63-89f98c59d882"
      },
      "source": [
        "stem_words= ['give','giving','given','gave']\n",
        "for words in stem_words:\n",
        "  print(words + \":\" + word_lemma.lemmatize(words))  "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "give:give\n",
            "giving:giving\n",
            "given:given\n",
            "gave:gave\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlpcWJUsIjMz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "da31b0f8-5480-4d24-9545-3848128a8b16"
      },
      "source": [
        "stem_words= ['give','giving','given','gave']\n",
        "for words in stem_words:\n",
        "  print(words + \":\" + word_lemma.lemmatize(words, 'v'))  "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "give:give\n",
            "giving:give\n",
            "given:give\n",
            "gave:give\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSd1ZTemOSR_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "75d58807-dad2-421d-e1a9-aa3eb50b2266"
      },
      "source": [
        "# import these modules \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "  \n",
        "lemmatizer = WordNetLemmatizer() \n",
        "  \n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
        "  \n",
        "# a denotes adjective in \"pos\" \n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rocks : rock\n",
            "corpora : corpus\n",
            "better : good\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWzTj7DBIqBx",
        "colab_type": "text"
      },
      "source": [
        "##StopWords##\n",
        "1. I, am, the, if.............</br>\n",
        "2. Not helpful in the processing of the language </br>\n",
        "2. Only helful in creating the sentence </br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjSpozDkI8DR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ugMxuSLJGy6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c4083ccc-c34e-429a-cb31-57effc9fd250"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amKxnUIwI_r5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "784a534a-551d-4aff-cfc2-24f12f0c2a2e"
      },
      "source": [
        "stopwords.words('english')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOPMZn2OJR22",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2722291-6f2b-46fd-d191-8a93ddce8f8a"
      },
      "source": [
        "len(stopwords.words('english'))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S_ejBP-Jepc",
        "colab_type": "text"
      },
      "source": [
        "####Regular Expression ####\n",
        "1. Sequence of characters that define search pattern </br>\n",
        "2. We use these for any desired ouput extraction say words with numbers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxDyTkW7J08q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "punctuation = re.compile(r'[-.?!,:;#()|0-9]')"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM0-1KM_KCmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "post_punct = []\n",
        "\n",
        "for words in tokens:\n",
        "    word = punctuation.sub(\"\", words)\n",
        "    if len(word)>0:\n",
        "      post_punct.append(word)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xMMv6WIKUTz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "9921c24f-2279-47d8-a936-a3887e5e74a0"
      },
      "source": [
        "post_punct"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We',\n",
              " 'all',\n",
              " 'are',\n",
              " 'in',\n",
              " 'the',\n",
              " 'Techademy',\n",
              " 'class',\n",
              " 'And',\n",
              " 'this',\n",
              " 'class',\n",
              " 'belongs',\n",
              " 'to',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'We',\n",
              " 'are',\n",
              " 'learning',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'including',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'NLP',\n",
              " 'deep',\n",
              " 'learning',\n",
              " 'computer',\n",
              " 'vision',\n",
              " 'and',\n",
              " 'various',\n",
              " 'other',\n",
              " 'applications',\n",
              " 'of',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'This',\n",
              " 'course',\n",
              " 'is',\n",
              " 'useful']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhwBRDqlKthn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d9e9fae-20d3-48cb-ce36-72c77136d96a"
      },
      "source": [
        "\n",
        "len(post_punct)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqwkkJVvKyuo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3011ab6d-aafc-4248-983b-0cc72c01ee73"
      },
      "source": [
        "len(tokens)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex2-nmK9LTL3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "2efb34b7-5a68-4b6c-e4fb-d059870f92bb"
      },
      "source": [
        "#Alternative approach\n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "  \n",
        "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "  \n",
        "stop_words = set(stopwords.words('english')) \n",
        "  \n",
        "word_tokens = word_tokenize(example_sent) \n",
        "  \n",
        "filtered_sentence = [] \n",
        "  \n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        filtered_sentence.append(w) \n",
        "  \n",
        "print(word_tokens) \n",
        "print(\"length of original sentence: \", len(word_tokens))\n",
        "print(filtered_sentence)\n",
        "print(\"length of filtered sentence: \", len(filtered_sentence))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "length of original sentence:  13\n",
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n",
            "length of filtered sentence:  9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmUJRMvyLwyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered_sentence_alternate = [w for w in word_tokens if not w in stop_words] \n",
        "print(filtered_sentence_alternate)\n",
        "print(\"length of altenate filtered sentence: \", len(filtered_sentence_alternate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwJ7ImiLOmV9",
        "colab_type": "text"
      },
      "source": [
        "## Name Entity Recognition ##\n",
        "\n",
        "1. Noun phrase Identification \n",
        "2. realted to POS classification; POS tags is required\n",
        "3. Ex: In Techademy, Dilanga teach AI at ZOOM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldM_CxVsPHY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import ne_chunk"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C6jJcmsPNd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_1 = \"The US President stays in White House\"\n",
        "sent_1_token = word_tokenize(sent_1)\n",
        "sent_1_tags = nltk.pos_tag(sent_1_token)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GoJm7TxPpEk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ce8369c1-8d9a-48f9-dbaf-5e406dfb2a47"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFs8b6ZKPsKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f1066099-1c1c-4e75-e056-365da87abceb"
      },
      "source": [
        "nltk.download('words')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLdxse6lPc5Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "8303d267-db4c-498a-abc9-fc250816ce6d"
      },
      "source": [
        "NER = ne_chunk(sent_1_tags)\n",
        "print(NER)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  The/DT\n",
            "  (ORGANIZATION US/NNP)\n",
            "  President/NNP\n",
            "  stays/VBZ\n",
            "  in/IN\n",
            "  (FACILITY White/NNP House/NNP))\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}